{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "# sys.path.insert allows us to define paths from where we can import .py files\n",
    "sys.path.insert(0, 'D://Users//jvdputte//Documents//Work//Educational//5LSM0//jupyter_scripts//assets') \n",
    "\n",
    "import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download and save the MNIST dataset. Also, we will create our 'handwritten' folder for the final question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mnist' has no attribute 'init'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mnist\u001b[39m.\u001b[39;49minit()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mnist' has no attribute 'init'"
     ]
    }
   ],
   "source": [
    "mnist.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and inspect its dimensions. Make sure you understand what each dimension represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = mnist.load()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot some examples of the handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what different samples of the same digit looks like. You can change the label to look at other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = X_train[y_train == 5][i].reshape(28, 28) # change the label to look at other digits\n",
    "    ax[i].imshow(img, cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('images/12_6.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Evaluate several examples of the digits. Name three data-augmentation operations that make sense for this data set.\n",
    "\n",
    "<span style=\"color:red\">''' Write down the answer in the cell markdown here (doubleclick the text)  '''</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the shape of the training data, we can see that the data is stored in vectorized form (The 28x28 pixel images are reshaped to a 1x784 array before storing). From the original 60.000 samples. The resulting shapes of the training data and training labels is what we expect. Performing sanity checks such as this is generally a good idea, it reduces the amount spent debugging when an inevitable bug shows up.\n",
    "\n",
    "### Q2: Think of two other sanity checks you could perform (what expectations do we have about our data). Data can be ordinary images but also auxilary information such as for example the age of a patient with accompanying CT scan.\n",
    "\n",
    "<span style=\"color:red\">''' Write down the answer in the cell markdown here (doubleclick the text)  '''</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron\n",
    "In this section we are going to implement our own multi-layer perceptron, including backpropagation. Carefully read all the hints in the code. The supplementary file will elaborate more on the details of the algorithm. You have to implement the following things:\n",
    "\n",
    "1. the weight matrix initalizations\n",
    "2. one-hot encoding\n",
    "3. the sigmoid function\n",
    "4. the forward pass\n",
    "5. the cross-entropy cost function\n",
    "6. the function that predicts (it uses the forward pass function and chooses the most likely prediction)\n",
    "7. the function that fits the model on the data (it consist of the forwards pass, backpropagation algorithm and L2-regularization)\n",
    "\n",
    "Before you start, really take your time to think about how your model should look and behave. What should the dimensions be of my weight and bias matrices? Why do I apply an activation function? How does one-hot encoding work with cross-entropy? It helps to also intermediatly print the sizes or values of you matrices to debug and confirm your code is working. For this, we have provided so called \"sanity check\" cells below the class cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l2 : float (default: 0.)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0. (default)\n",
    "    epochs : int (default: 100)\n",
    "        Number of passes over the training set.\n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "    minibatch_size : int (default: 1)\n",
    "        Number of training samples per minibatch.\n",
    "    seed : int (default: None)\n",
    "        Random seed for initalizing weights and shuffling.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    eval_ : dict\n",
    "      Dictionary collecting the cost, training accuracy,\n",
    "      and validation accuracy for each epoch during training.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=786, n_hidden=30, n_classes=10,\n",
    "                 l2=0., epochs=100, eta=0.001,\n",
    "                 shuffle=True, minibatch_size=1, seed=None):\n",
    "\n",
    "        self.random = np.random.RandomState(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        # insert the proper dimensions for the initialization matrices. Use the class variables.\n",
    "        self.b_h = np.random.normal(size=( ))\n",
    "        self.b_out = np.random.normal(size=( ))\n",
    "        self.w_h = np.random.normal(size=( , ))\n",
    "        self.w_out = np.random.normal(size=( , ))\n",
    "                \n",
    "    \n",
    "    # the onehot function encodes labels into onehot representation e.g. with 3 labels, 0->[1,0,0], 1->[0,1,0], 2->[0,0,1]\n",
    "    def _onehot(self, y, n_classes):\n",
    "        \"\"\"Encode labels into one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_samples, n_labels)\n",
    "\n",
    "        \"\"\"\n",
    "        # implement onehot encoding here. \n",
    "\n",
    "        # HINT: We would like a n_samples x n_labels vector where each row\n",
    "        # represents a sample vector with zeros everywhere except for position k, where index k equals the target value (0-9).\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\"\"\"\n",
    "        # implement sigmoid function here\n",
    "        \n",
    "        return sigmoid_out\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"Compute forward propagation step\"\"\"\n",
    "\n",
    "        # step 1: net input of hidden layer\n",
    "        # [n_samples, n_features] dot [n_features, n_hidden]-> [n_samples, n_hidden]\n",
    "\n",
    "\n",
    "        # step 2: activation of hidden layer\n",
    "\n",
    "\n",
    "        # step 3: net input of output layer\n",
    "        # [n_samples, n_hidden] dot [n_hidden, n_classlabels]\n",
    "        # -> [n_samples, n_classlabels]\n",
    "\n",
    "\n",
    "        # step 4: activation output layer\n",
    "\n",
    "\n",
    "        return z_h, a_h, z_out, a_out\n",
    "\n",
    "    def _compute_cost(self, y_enc, output):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_samples, n_labels)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_samples, n_output_units]\n",
    "            Activation of the output layer (forward propagation)\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # calculate the cross-entropy cost \n",
    "        \n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # implement prediction here\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X_train : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y_train : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        X_valid : array, shape = [n_samples, n_features]\n",
    "            Sample features for validation during training\n",
    "        y_valid : array, shape = [n_samples]\n",
    "            Sample labels for validation during training\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        n_output = np.unique(y_train).shape[0]  # number of class labels\n",
    "        n_features = X_train.shape[1]\n",
    "\n",
    "        ########################\n",
    "        # Weight initialization\n",
    "        ########################\n",
    "\n",
    "        # weights for input -> hidden\n",
    "        self.b_h = np.zeros(self.n_hidden)\n",
    "        self.w_h = self.random.normal(loc=0.0, scale=0.1,\n",
    "                                      size=(n_features, self.n_hidden))\n",
    "\n",
    "        # weights for hidden -> output\n",
    "        self.b_out = np.zeros(n_output)\n",
    "        self.w_out = self.random.normal(loc=0.0, scale=0.1,\n",
    "                                        size=(self.n_hidden, n_output))\n",
    "        \n",
    "        # create dictionary to keep track of training scores\n",
    "        self.eval_ = {'cost': [], 'train_acc': [], 'valid_acc': []}\n",
    "        \n",
    "        # convert y to one-hot encoding\n",
    "        y_train_enc = self._onehot(y_train, n_output)\n",
    "\n",
    "        # iterate over training epochs\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # iterate over minibatches\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "\n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "            \n",
    "            # this for loop takes care of the batching process. Every iteration several samples are fed through the network in one go.\n",
    "            for start_idx in range(0, indices.shape[0] - self.minibatch_size +\n",
    "                                   1, self.minibatch_size):\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                z_h, a_h, z_out, a_out = self._forward(X_train[batch_idx])\n",
    "\n",
    "                ##################\n",
    "                # Backpropagation\n",
    "                ##################\n",
    "\n",
    "                # calculate error vector of the output layer: sigma_out, [n_samples, n_classlabels]\n",
    "                \n",
    "\n",
    "                \n",
    "                # calculate derivative of the sigmoid activation function:\n",
    "                # sigmoid_derivative_h, [n_samples, n_hidden]\n",
    "                \n",
    "\n",
    "                # Calculate the error term of the hidden layer: sigma_h, \n",
    "                #[n_samples, n_classlabels] dot [n_classlabels, n_hidden] -> [n_samples, n_hidden]\n",
    "\n",
    "\n",
    "                # Calculate the gradients of the weights in the hidden layer\n",
    "                # [n_features, n_samples] dot [n_samples, n_hidden] -> [n_features, n_hidden]\n",
    "\n",
    "                # calculate gradients of the biases in the hidden layer (freebee)\n",
    "                grad_b_h = np.sum(sigma_h, axis=0)\n",
    "\n",
    "                # Calculate the gradients of the weights in the output layer\n",
    "                # [n_hidden, n_samples] dot [n_samples, n_classlabels] -> [n_hidden, n_classlabels]\n",
    "                \n",
    "                # calculate the gradient of the biases in the output layer\n",
    "                grad_b_out = np.sum(sigma_out, axis=0)\n",
    "\n",
    "                # L2-regularization and weight updates (bias is not regularized)\n",
    "                delta_w_h = \n",
    "                delta_b_h = grad_b_h \n",
    "                self.w_h -= self.eta * delta_w_h\n",
    "                self.b_h -= self.eta * delta_b_h\n",
    "\n",
    "                delta_w_out = \n",
    "                delta_b_out = grad_b_out\n",
    "                self.w_out -= self.eta * delta_w_out\n",
    "                self.b_out -= self.eta * delta_b_out\n",
    "\n",
    "            #############\n",
    "            # Evaluation\n",
    "            #############\n",
    "\n",
    "            # Evaluation after each epoch during training\n",
    "            z_h, a_h, z_out, a_out = self._forward(X_train)\n",
    "            \n",
    "            cost = self._compute_cost(y_enc=y_train_enc,\n",
    "                                      output=a_out)\n",
    "\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            y_valid_pred = self.predict(X_valid)\n",
    "\n",
    "            train_acc = ((np.sum(y_train == y_train_pred)).astype(np.float) /\n",
    "                         X_train.shape[0])\n",
    "            valid_acc = ((np.sum(y_valid == y_valid_pred)).astype(np.float) /\n",
    "                         X_valid.shape[0])\n",
    "\n",
    "            sys.stderr.write('\\r' + '{}/{} | Cost: {:.1f} | Train/Valid Acc.: {:.2f}%/{:.2f}% '.format(i+1, self.epochs, cost,\n",
    "                              train_acc*100, valid_acc*100))\n",
    "            sys.stderr.flush()\n",
    "\n",
    "            self.eval_['cost'].append(cost)\n",
    "            self.eval_['train_acc'].append(train_acc)\n",
    "            self.eval_['valid_acc'].append(valid_acc)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! SANITY CHECK AREA ! - You can check here wether your functions work as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = NeuralNetMLP()\n",
    "\n",
    "# Onehot encoding\n",
    "labels = np.array([0., 3., 4., 5., 9., 4., 0.])\n",
    "n_classes = 10\n",
    "onehot_vectors = test_model._onehot(labels, n_classes)\n",
    "print(onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation\n",
    "x = np.linspace(-10, 10, 100)\n",
    "out = test_model._sigmoid(x)\n",
    "plt.plot(out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward call\n",
    "samples = np.random.normal(size=(50, 786)) # fake samples\n",
    "z_h, a_h, z_out, a_out = test_model._forward(samples)\n",
    "print(z_h.shape, a_h.shape, z_out.shape, a_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss (cost function)\n",
    "outputs = np.random.rand(7, 10)\n",
    "loss = test_model._compute_cost(onehot_vectors, outputs)\n",
    "print(loss) # should appr. be between 5 and 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions - the predictions make no sense at this point of course!\n",
    "predictions = test_model.predict(samples)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize the network and feed it training and validation data (55000 samples are used for training, remaining 5000 samples are used for validation).\n",
    "### Q3. Why do we define a validation set even though we already have a test set of unseen samples? Explain your answer.\n",
    "<span style=\"color:red\">''' Write down the answer in the cell markdown here (doubleclick the text)  '''</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(n_hidden=100, \n",
    "                  l2=0.0001, \n",
    "                  epochs=20, \n",
    "                  eta=0.0005,\n",
    "                  minibatch_size=100, \n",
    "                  shuffle=True,\n",
    "                  seed=1)\n",
    "\n",
    "nn.fit(X_train=X_train[:50000], \n",
    "       y_train=y_train[:50000],\n",
    "       X_valid=X_train[50000:],\n",
    "       y_valid=y_train[50000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the cost and the training/validation accuracy for 20 epochs. We can see the cost is decreasing and accuracy of both the training and validation is going up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.plot(range(nn.epochs), nn.eval_['cost'])\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlabel('Epochs')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "ax.plot(range(nn.epochs), nn.eval_['train_acc'], \n",
    "         label='training')\n",
    "ax.plot(range(nn.epochs), nn.eval_['valid_acc'], \n",
    "         label='validation', linestyle='--')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. The validation accuracy is higher than the training accuracy. What does this say about the quality of the model?\n",
    "<span style=\"color:red\">''' Write down the answer in the cell markdown here (doubleclick the text)  '''</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(n_hidden=100, \n",
    "                  l2=0.0001, \n",
    "                  epochs=200, \n",
    "                  eta=0.0005,\n",
    "                  minibatch_size=100, \n",
    "                  shuffle=True,\n",
    "                  seed=123)\n",
    "\n",
    "nn.fit(X_train=X_train[:50000], \n",
    "       y_train=y_train[:50000],\n",
    "       X_valid=X_train[50000:],\n",
    "       y_valid=y_train[50000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(1,2,1)\n",
    "ax.plot(range(nn.epochs), nn.eval_['cost'])\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_xlabel('Epochs')\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "ax.plot(range(nn.epochs), nn.eval_['train_acc'], \n",
    "         label='training')\n",
    "ax.plot(range(nn.epochs), nn.eval_['valid_acc'], \n",
    "         label='validation', linestyle='--')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. The neural network is getting pretty good results on the data. What changes in the architecture or training procedure could you change in order to improve on the results further?\n",
    "\n",
    "<span style=\"color:red\">''' Write down the answer in the cell markdown here (doubleclick the text)  '''</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. We used accuracy as our evaluation metric but there are many other metrics we can evaluate on. Explain the possible pitfalls of only relying on accuracy with an example. Suggest other metrics we can use to see if our model generalized well and explain what the metric evaluates on.\n",
    "\n",
    "<span style=\"color:red\">''' Write down the answer in the cell markdown here (doubleclick the text)  '''</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. We are now going to run inference with our own handwritten images. Create three replicas of MNIST digits (by using e.g. paint) and show what your model predicts.   \n",
    "\n",
    "Put your png images in the 'handwritten/' folder. You can simply use paint to write on a 28x28 canvas. Try to match the images as closely to MNIST-style as possible. \n",
    "\n",
    "For information on MNIST: http://yann.lecun.com/exdb/mnist/ \n",
    "\n",
    "Matplotlib shows the images black on white but the images are actually <b>white on black!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hw = mnist.handwritten()  # should be (N, 28, 28)\n",
    "\n",
    "# Print handwritten digits\n",
    "fig, ax = plt.subplots(nrows=1, ncols=X_hw.shape[0], sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range((X_hw.shape[0])):\n",
    "    ax[i].imshow(X_hw[i,...], cmap='Greys')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference here and present predictions. \n",
    "\n",
    "# First reshape numpy array as our training data\n",
    "\n",
    "# Make predictions and present results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5lsm0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7693cff941168a4e2d00678936d0b60c6344764cb517bf8dd169736bbca454c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
